// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.30.0
// source: upload_history.sql

package db

import (
	"context"

	"github.com/jackc/pgx/v5/pgtype"
)

const batchInsertTransactions = `-- name: BatchInsertTransactions :many
INSERT INTO transactions (
    user_id,
    upload_id,
    txn_date,
    description,
    amount,
    txn_type,
    category,
    is_reviewed,
    raw_data
) VALUES (
    unnest($1::UUID[]),
    unnest($2::UUID[]),
    unnest($3::DATE[]),
    unnest($4::TEXT[]),
    unnest($5::DECIMAL[]),
    unnest($6::VARCHAR[]),
    unnest($7::VARCHAR[]),
    unnest($8::BOOLEAN[]),
    unnest($9::TEXT[])
)
ON CONFLICT (user_id, txn_date, description, amount) DO NOTHING
RETURNING id, user_id, txn_date, description, amount, txn_type, category, is_reviewed, raw_data, created_at, updated_at, upload_id
`

type BatchInsertTransactionsParams struct {
	UserIds      []pgtype.UUID    `json:"user_ids"`
	UploadIds    []pgtype.UUID    `json:"upload_ids"`
	TxnDates     []pgtype.Date    `json:"txn_dates"`
	Descriptions []string         `json:"descriptions"`
	Amounts      []pgtype.Numeric `json:"amounts"`
	TxnTypes     []string         `json:"txn_types"`
	Categories   []string         `json:"categories"`
	IsReviewed   []bool           `json:"is_reviewed"`
	RawData      []string         `json:"raw_data"`
}

// Batch insert with ON CONFLICT for duplicate handling (for smaller batches)
func (q *Queries) BatchInsertTransactions(ctx context.Context, arg BatchInsertTransactionsParams) ([]Transaction, error) {
	rows, err := q.db.Query(ctx, batchInsertTransactions,
		arg.UserIds,
		arg.UploadIds,
		arg.TxnDates,
		arg.Descriptions,
		arg.Amounts,
		arg.TxnTypes,
		arg.Categories,
		arg.IsReviewed,
		arg.RawData,
	)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []Transaction{}
	for rows.Next() {
		var i Transaction
		if err := rows.Scan(
			&i.ID,
			&i.UserID,
			&i.TxnDate,
			&i.Description,
			&i.Amount,
			&i.TxnType,
			&i.Category,
			&i.IsReviewed,
			&i.RawData,
			&i.CreatedAt,
			&i.UpdatedAt,
			&i.UploadID,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

type BulkInsertTransactionsParams struct {
	UserID      pgtype.UUID    `json:"user_id"`
	UploadID    pgtype.UUID    `json:"upload_id"`
	TxnDate     pgtype.Date    `json:"txn_date"`
	Description string         `json:"description"`
	Amount      pgtype.Numeric `json:"amount"`
	TxnType     string         `json:"txn_type"`
	Category    pgtype.Text    `json:"category"`
	IsReviewed  bool           `json:"is_reviewed"`
	RawData     pgtype.Text    `json:"raw_data"`
}

const checkBatchDuplicates = `-- name: CheckBatchDuplicates :many
SELECT check_batch_duplicates FROM check_batch_duplicates($1, $2)
`

type CheckBatchDuplicatesParams struct {
	PUserID       pgtype.UUID `json:"p_user_id"`
	PTransactions []byte      `json:"p_transactions"`
}

// Check multiple transactions for duplicates (returns list of duplicates)
func (q *Queries) CheckBatchDuplicates(ctx context.Context, arg CheckBatchDuplicatesParams) ([]interface{}, error) {
	rows, err := q.db.Query(ctx, checkBatchDuplicates, arg.PUserID, arg.PTransactions)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []interface{}{}
	for rows.Next() {
		var check_batch_duplicates interface{}
		if err := rows.Scan(&check_batch_duplicates); err != nil {
			return nil, err
		}
		items = append(items, check_batch_duplicates)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const checkTransactionDuplicate = `-- name: CheckTransactionDuplicate :one

SELECT check_transaction_duplicate($1, $2, $3, $4) as is_duplicate
`

type CheckTransactionDuplicateParams struct {
	PUserID      pgtype.UUID    `json:"p_user_id"`
	PTxnDate     pgtype.Date    `json:"p_txn_date"`
	PDescription string         `json:"p_description"`
	PAmount      pgtype.Numeric `json:"p_amount"`
}

// ============================================
// Duplicate Detection Queries
// ============================================
// Check if a transaction is a duplicate
func (q *Queries) CheckTransactionDuplicate(ctx context.Context, arg CheckTransactionDuplicateParams) (bool, error) {
	row := q.db.QueryRow(ctx, checkTransactionDuplicate,
		arg.PUserID,
		arg.PTxnDate,
		arg.PDescription,
		arg.PAmount,
	)
	var is_duplicate bool
	err := row.Scan(&is_duplicate)
	return is_duplicate, err
}

const completeUploadProcessing = `-- name: CompleteUploadProcessing :one
UPDATE upload_history
SET
    status = $2,
    error_message = $3,
    processing_completed_at = NOW(),
    total_rows = $4,
    parsed_rows = $5,
    categorized_rows = $6,
    duplicate_rows = $7,
    error_rows = $8,
    updated_at = NOW()
WHERE id = $1
RETURNING id, user_id, filename, file_key, file_size_bytes, file_hash, bank_type, status, error_message, processing_started_at, processing_completed_at, total_rows, parsed_rows, categorized_rows, duplicate_rows, error_rows, accuracy_percent, processing_duration_ms, created_at, updated_at
`

type CompleteUploadProcessingParams struct {
	ID              pgtype.UUID  `json:"id"`
	Status          UploadStatus `json:"status"`
	ErrorMessage    pgtype.Text  `json:"error_message"`
	TotalRows       pgtype.Int4  `json:"total_rows"`
	ParsedRows      pgtype.Int4  `json:"parsed_rows"`
	CategorizedRows pgtype.Int4  `json:"categorized_rows"`
	DuplicateRows   pgtype.Int4  `json:"duplicate_rows"`
	ErrorRows       pgtype.Int4  `json:"error_rows"`
}

// Mark upload as completed with statistics
func (q *Queries) CompleteUploadProcessing(ctx context.Context, arg CompleteUploadProcessingParams) (UploadHistory, error) {
	row := q.db.QueryRow(ctx, completeUploadProcessing,
		arg.ID,
		arg.Status,
		arg.ErrorMessage,
		arg.TotalRows,
		arg.ParsedRows,
		arg.CategorizedRows,
		arg.DuplicateRows,
		arg.ErrorRows,
	)
	var i UploadHistory
	err := row.Scan(
		&i.ID,
		&i.UserID,
		&i.Filename,
		&i.FileKey,
		&i.FileSizeBytes,
		&i.FileHash,
		&i.BankType,
		&i.Status,
		&i.ErrorMessage,
		&i.ProcessingStartedAt,
		&i.ProcessingCompletedAt,
		&i.TotalRows,
		&i.ParsedRows,
		&i.CategorizedRows,
		&i.DuplicateRows,
		&i.ErrorRows,
		&i.AccuracyPercent,
		&i.ProcessingDurationMs,
		&i.CreatedAt,
		&i.UpdatedAt,
	)
	return i, err
}

const countDuplicatesByUpload = `-- name: CountDuplicatesByUpload :one
SELECT COUNT(*) as duplicate_count
FROM (
    SELECT DISTINCT t1.id
    FROM transactions t1
    JOIN transactions t2 ON
        t1.user_id = t2.user_id AND
        t1.txn_date = t2.txn_date AND
        t1.description = t2.description AND
        t1.amount = t2.amount AND
        t1.id != t2.id
    WHERE t1.upload_id = $1 AND t2.upload_id != $1
) as duplicates
`

// Count duplicate transactions for an upload
func (q *Queries) CountDuplicatesByUpload(ctx context.Context, uploadID pgtype.UUID) (int64, error) {
	row := q.db.QueryRow(ctx, countDuplicatesByUpload, uploadID)
	var duplicate_count int64
	err := row.Scan(&duplicate_count)
	return duplicate_count, err
}

const countTransactionsByUpload = `-- name: CountTransactionsByUpload :one
SELECT
    COUNT(*) as total,
    COUNT(CASE WHEN category IS NOT NULL THEN 1 END) as categorized,
    COUNT(CASE WHEN category IS NULL THEN 1 END) as uncategorized
FROM transactions
WHERE upload_id = $1
`

type CountTransactionsByUploadRow struct {
	Total         int64 `json:"total"`
	Categorized   int64 `json:"categorized"`
	Uncategorized int64 `json:"uncategorized"`
}

// Count transactions for an upload
func (q *Queries) CountTransactionsByUpload(ctx context.Context, uploadID pgtype.UUID) (CountTransactionsByUploadRow, error) {
	row := q.db.QueryRow(ctx, countTransactionsByUpload, uploadID)
	var i CountTransactionsByUploadRow
	err := row.Scan(&i.Total, &i.Categorized, &i.Uncategorized)
	return i, err
}

const createUploadHistory = `-- name: CreateUploadHistory :one

INSERT INTO upload_history (
    user_id,
    filename,
    file_key,
    file_size_bytes,
    file_hash,
    bank_type,
    status,
    total_rows
) VALUES (
    $1, $2, $3, $4, $5, $6, $7, $8
)
RETURNING id, user_id, filename, file_key, file_size_bytes, file_hash, bank_type, status, error_message, processing_started_at, processing_completed_at, total_rows, parsed_rows, categorized_rows, duplicate_rows, error_rows, accuracy_percent, processing_duration_ms, created_at, updated_at
`

type CreateUploadHistoryParams struct {
	UserID        pgtype.UUID  `json:"user_id"`
	Filename      string       `json:"filename"`
	FileKey       string       `json:"file_key"`
	FileSizeBytes pgtype.Int8  `json:"file_size_bytes"`
	FileHash      pgtype.Text  `json:"file_hash"`
	BankType      pgtype.Text  `json:"bank_type"`
	Status        UploadStatus `json:"status"`
	TotalRows     pgtype.Int4  `json:"total_rows"`
}

// SQLC queries for upload_history table
// Create a new upload history record
func (q *Queries) CreateUploadHistory(ctx context.Context, arg CreateUploadHistoryParams) (UploadHistory, error) {
	row := q.db.QueryRow(ctx, createUploadHistory,
		arg.UserID,
		arg.Filename,
		arg.FileKey,
		arg.FileSizeBytes,
		arg.FileHash,
		arg.BankType,
		arg.Status,
		arg.TotalRows,
	)
	var i UploadHistory
	err := row.Scan(
		&i.ID,
		&i.UserID,
		&i.Filename,
		&i.FileKey,
		&i.FileSizeBytes,
		&i.FileHash,
		&i.BankType,
		&i.Status,
		&i.ErrorMessage,
		&i.ProcessingStartedAt,
		&i.ProcessingCompletedAt,
		&i.TotalRows,
		&i.ParsedRows,
		&i.CategorizedRows,
		&i.DuplicateRows,
		&i.ErrorRows,
		&i.AccuracyPercent,
		&i.ProcessingDurationMs,
		&i.CreatedAt,
		&i.UpdatedAt,
	)
	return i, err
}

const deleteUploadHistory = `-- name: DeleteUploadHistory :exec
DELETE FROM upload_history
WHERE id = $1
`

// Delete an upload history record (cascade will handle transactions)
func (q *Queries) DeleteUploadHistory(ctx context.Context, id pgtype.UUID) error {
	_, err := q.db.Exec(ctx, deleteUploadHistory, id)
	return err
}

const deleteUserUploadHistory = `-- name: DeleteUserUploadHistory :exec
DELETE FROM upload_history
WHERE user_id = $1
`

// Delete all upload history for a user
func (q *Queries) DeleteUserUploadHistory(ctx context.Context, userID pgtype.UUID) error {
	_, err := q.db.Exec(ctx, deleteUserUploadHistory, userID)
	return err
}

const getDuplicateTransactions = `-- name: GetDuplicateTransactions :many
SELECT
    t1.id as transaction_id,
    t1.txn_date,
    t1.description,
    t1.amount,
    t2.id as duplicate_id,
    t2.created_at as duplicate_created_at
FROM transactions t1
JOIN transactions t2 ON
    t1.user_id = t2.user_id AND
    t1.txn_date = t2.txn_date AND
    t1.description = t2.description AND
    t1.amount = t2.amount AND
    t1.id != t2.id
WHERE t1.user_id = $1
ORDER BY t1.txn_date DESC, t1.created_at DESC
`

type GetDuplicateTransactionsRow struct {
	TransactionID      pgtype.UUID        `json:"transaction_id"`
	TxnDate            pgtype.Date        `json:"txn_date"`
	Description        string             `json:"description"`
	Amount             pgtype.Numeric     `json:"amount"`
	DuplicateID        pgtype.UUID        `json:"duplicate_id"`
	DuplicateCreatedAt pgtype.Timestamptz `json:"duplicate_created_at"`
}

// Find potential duplicate transactions for a user
func (q *Queries) GetDuplicateTransactions(ctx context.Context, userID pgtype.UUID) ([]GetDuplicateTransactionsRow, error) {
	rows, err := q.db.Query(ctx, getDuplicateTransactions, userID)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []GetDuplicateTransactionsRow{}
	for rows.Next() {
		var i GetDuplicateTransactionsRow
		if err := rows.Scan(
			&i.TransactionID,
			&i.TxnDate,
			&i.Description,
			&i.Amount,
			&i.DuplicateID,
			&i.DuplicateCreatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getProcessingUploads = `-- name: GetProcessingUploads :many
SELECT id, user_id, filename, file_key, file_size_bytes, file_hash, bank_type, status, error_message, processing_started_at, processing_completed_at, total_rows, parsed_rows, categorized_rows, duplicate_rows, error_rows, accuracy_percent, processing_duration_ms, created_at, updated_at FROM upload_history
WHERE status IN ('pending', 'processing')
ORDER BY created_at ASC
`

// Get all uploads currently being processed (for monitoring)
func (q *Queries) GetProcessingUploads(ctx context.Context) ([]UploadHistory, error) {
	rows, err := q.db.Query(ctx, getProcessingUploads)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []UploadHistory{}
	for rows.Next() {
		var i UploadHistory
		if err := rows.Scan(
			&i.ID,
			&i.UserID,
			&i.Filename,
			&i.FileKey,
			&i.FileSizeBytes,
			&i.FileHash,
			&i.BankType,
			&i.Status,
			&i.ErrorMessage,
			&i.ProcessingStartedAt,
			&i.ProcessingCompletedAt,
			&i.TotalRows,
			&i.ParsedRows,
			&i.CategorizedRows,
			&i.DuplicateRows,
			&i.ErrorRows,
			&i.AccuracyPercent,
			&i.ProcessingDurationMs,
			&i.CreatedAt,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getRecentUploads = `-- name: GetRecentUploads :many
SELECT id, user_id, filename, file_key, file_size_bytes, file_hash, bank_type, status, error_message, processing_started_at, processing_completed_at, total_rows, parsed_rows, categorized_rows, duplicate_rows, error_rows, accuracy_percent, processing_duration_ms, created_at, updated_at FROM upload_history
WHERE user_id = $1
ORDER BY created_at DESC
LIMIT 10
`

// Get recent uploads for a user (last 10)
func (q *Queries) GetRecentUploads(ctx context.Context, userID pgtype.UUID) ([]UploadHistory, error) {
	rows, err := q.db.Query(ctx, getRecentUploads, userID)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []UploadHistory{}
	for rows.Next() {
		var i UploadHistory
		if err := rows.Scan(
			&i.ID,
			&i.UserID,
			&i.Filename,
			&i.FileKey,
			&i.FileSizeBytes,
			&i.FileHash,
			&i.BankType,
			&i.Status,
			&i.ErrorMessage,
			&i.ProcessingStartedAt,
			&i.ProcessingCompletedAt,
			&i.TotalRows,
			&i.ParsedRows,
			&i.CategorizedRows,
			&i.DuplicateRows,
			&i.ErrorRows,
			&i.AccuracyPercent,
			&i.ProcessingDurationMs,
			&i.CreatedAt,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getStuckUploads = `-- name: GetStuckUploads :many
SELECT id, user_id, filename, file_key, file_size_bytes, file_hash, bank_type, status, error_message, processing_started_at, processing_completed_at, total_rows, parsed_rows, categorized_rows, duplicate_rows, error_rows, accuracy_percent, processing_duration_ms, created_at, updated_at FROM upload_history
WHERE status = 'processing'
  AND processing_started_at < NOW() - INTERVAL '5 minutes'
ORDER BY processing_started_at ASC
`

// Find uploads stuck in processing state (older than 5 minutes)
func (q *Queries) GetStuckUploads(ctx context.Context) ([]UploadHistory, error) {
	rows, err := q.db.Query(ctx, getStuckUploads)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []UploadHistory{}
	for rows.Next() {
		var i UploadHistory
		if err := rows.Scan(
			&i.ID,
			&i.UserID,
			&i.Filename,
			&i.FileKey,
			&i.FileSizeBytes,
			&i.FileHash,
			&i.BankType,
			&i.Status,
			&i.ErrorMessage,
			&i.ProcessingStartedAt,
			&i.ProcessingCompletedAt,
			&i.TotalRows,
			&i.ParsedRows,
			&i.CategorizedRows,
			&i.DuplicateRows,
			&i.ErrorRows,
			&i.AccuracyPercent,
			&i.ProcessingDurationMs,
			&i.CreatedAt,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getTransactionsByUpload = `-- name: GetTransactionsByUpload :many
SELECT id, user_id, txn_date, description, amount, txn_type, category, is_reviewed, raw_data, created_at, updated_at, upload_id FROM transactions
WHERE upload_id = $1
ORDER BY txn_date DESC, created_at DESC
`

// Get all transactions from a specific upload
func (q *Queries) GetTransactionsByUpload(ctx context.Context, uploadID pgtype.UUID) ([]Transaction, error) {
	rows, err := q.db.Query(ctx, getTransactionsByUpload, uploadID)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []Transaction{}
	for rows.Next() {
		var i Transaction
		if err := rows.Scan(
			&i.ID,
			&i.UserID,
			&i.TxnDate,
			&i.Description,
			&i.Amount,
			&i.TxnType,
			&i.Category,
			&i.IsReviewed,
			&i.RawData,
			&i.CreatedAt,
			&i.UpdatedAt,
			&i.UploadID,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getUploadByFileHash = `-- name: GetUploadByFileHash :one
SELECT id, user_id, filename, file_key, file_size_bytes, file_hash, bank_type, status, error_message, processing_started_at, processing_completed_at, total_rows, parsed_rows, categorized_rows, duplicate_rows, error_rows, accuracy_percent, processing_duration_ms, created_at, updated_at FROM upload_history
WHERE user_id = $1 AND file_hash = $2
ORDER BY created_at DESC
LIMIT 1
`

type GetUploadByFileHashParams struct {
	UserID   pgtype.UUID `json:"user_id"`
	FileHash pgtype.Text `json:"file_hash"`
}

// Check if file has already been uploaded (duplicate detection)
func (q *Queries) GetUploadByFileHash(ctx context.Context, arg GetUploadByFileHashParams) (UploadHistory, error) {
	row := q.db.QueryRow(ctx, getUploadByFileHash, arg.UserID, arg.FileHash)
	var i UploadHistory
	err := row.Scan(
		&i.ID,
		&i.UserID,
		&i.Filename,
		&i.FileKey,
		&i.FileSizeBytes,
		&i.FileHash,
		&i.BankType,
		&i.Status,
		&i.ErrorMessage,
		&i.ProcessingStartedAt,
		&i.ProcessingCompletedAt,
		&i.TotalRows,
		&i.ParsedRows,
		&i.CategorizedRows,
		&i.DuplicateRows,
		&i.ErrorRows,
		&i.AccuracyPercent,
		&i.ProcessingDurationMs,
		&i.CreatedAt,
		&i.UpdatedAt,
	)
	return i, err
}

const getUploadHistoryByID = `-- name: GetUploadHistoryByID :one
SELECT id, user_id, filename, file_key, file_size_bytes, file_hash, bank_type, status, error_message, processing_started_at, processing_completed_at, total_rows, parsed_rows, categorized_rows, duplicate_rows, error_rows, accuracy_percent, processing_duration_ms, created_at, updated_at FROM upload_history
WHERE id = $1
LIMIT 1
`

// Get a single upload history record by ID
func (q *Queries) GetUploadHistoryByID(ctx context.Context, id pgtype.UUID) (UploadHistory, error) {
	row := q.db.QueryRow(ctx, getUploadHistoryByID, id)
	var i UploadHistory
	err := row.Scan(
		&i.ID,
		&i.UserID,
		&i.Filename,
		&i.FileKey,
		&i.FileSizeBytes,
		&i.FileHash,
		&i.BankType,
		&i.Status,
		&i.ErrorMessage,
		&i.ProcessingStartedAt,
		&i.ProcessingCompletedAt,
		&i.TotalRows,
		&i.ParsedRows,
		&i.CategorizedRows,
		&i.DuplicateRows,
		&i.ErrorRows,
		&i.AccuracyPercent,
		&i.ProcessingDurationMs,
		&i.CreatedAt,
		&i.UpdatedAt,
	)
	return i, err
}

const getUploadHistoryByUserAndID = `-- name: GetUploadHistoryByUserAndID :one
SELECT id, user_id, filename, file_key, file_size_bytes, file_hash, bank_type, status, error_message, processing_started_at, processing_completed_at, total_rows, parsed_rows, categorized_rows, duplicate_rows, error_rows, accuracy_percent, processing_duration_ms, created_at, updated_at FROM upload_history
WHERE id = $1 AND user_id = $2
LIMIT 1
`

type GetUploadHistoryByUserAndIDParams struct {
	ID     pgtype.UUID `json:"id"`
	UserID pgtype.UUID `json:"user_id"`
}

// Get upload history for a specific user and upload ID (for security)
func (q *Queries) GetUploadHistoryByUserAndID(ctx context.Context, arg GetUploadHistoryByUserAndIDParams) (UploadHistory, error) {
	row := q.db.QueryRow(ctx, getUploadHistoryByUserAndID, arg.ID, arg.UserID)
	var i UploadHistory
	err := row.Scan(
		&i.ID,
		&i.UserID,
		&i.Filename,
		&i.FileKey,
		&i.FileSizeBytes,
		&i.FileHash,
		&i.BankType,
		&i.Status,
		&i.ErrorMessage,
		&i.ProcessingStartedAt,
		&i.ProcessingCompletedAt,
		&i.TotalRows,
		&i.ParsedRows,
		&i.CategorizedRows,
		&i.DuplicateRows,
		&i.ErrorRows,
		&i.AccuracyPercent,
		&i.ProcessingDurationMs,
		&i.CreatedAt,
		&i.UpdatedAt,
	)
	return i, err
}

const getUploadPerformanceMetrics = `-- name: GetUploadPerformanceMetrics :one

SELECT
    AVG(processing_duration_ms) as avg_duration_ms,
    MIN(processing_duration_ms) as min_duration_ms,
    MAX(processing_duration_ms) as max_duration_ms,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY processing_duration_ms) as median_duration_ms,
    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY processing_duration_ms) as p95_duration_ms,
    AVG(total_rows) as avg_rows_per_upload,
    AVG(CASE WHEN total_rows > 0 THEN processing_duration_ms::FLOAT / total_rows ELSE 0 END) as avg_ms_per_row
FROM upload_history
WHERE user_id = $1
  AND status = 'completed'
  AND processing_duration_ms IS NOT NULL
`

type GetUploadPerformanceMetricsRow struct {
	AvgDurationMs    float64     `json:"avg_duration_ms"`
	MinDurationMs    interface{} `json:"min_duration_ms"`
	MaxDurationMs    interface{} `json:"max_duration_ms"`
	MedianDurationMs float64     `json:"median_duration_ms"`
	P95DurationMs    float64     `json:"p95_duration_ms"`
	AvgRowsPerUpload float64     `json:"avg_rows_per_upload"`
	AvgMsPerRow      float64     `json:"avg_ms_per_row"`
}

// ============================================
// Performance Monitoring Queries
// ============================================
// Get performance metrics for uploads
func (q *Queries) GetUploadPerformanceMetrics(ctx context.Context, userID pgtype.UUID) (GetUploadPerformanceMetricsRow, error) {
	row := q.db.QueryRow(ctx, getUploadPerformanceMetrics, userID)
	var i GetUploadPerformanceMetricsRow
	err := row.Scan(
		&i.AvgDurationMs,
		&i.MinDurationMs,
		&i.MaxDurationMs,
		&i.MedianDurationMs,
		&i.P95DurationMs,
		&i.AvgRowsPerUpload,
		&i.AvgMsPerRow,
	)
	return i, err
}

const getUploadStatsByUser = `-- name: GetUploadStatsByUser :one
SELECT
    COUNT(*) as total_uploads,
    COUNT(CASE WHEN status = 'completed' THEN 1 END) as successful_uploads,
    COUNT(CASE WHEN status = 'failed' THEN 1 END) as failed_uploads,
    COUNT(CASE WHEN status = 'processing' THEN 1 END) as processing_uploads,
    COUNT(CASE WHEN status = 'pending' THEN 1 END) as pending_uploads,
    COALESCE(SUM(total_rows), 0) as total_rows_processed,
    COALESCE(SUM(categorized_rows), 0) as total_categorized,
    COALESCE(AVG(accuracy_percent), 0) as avg_accuracy_percent,
    COALESCE(AVG(processing_duration_ms), 0) as avg_processing_ms
FROM upload_history
WHERE user_id = $1
`

type GetUploadStatsByUserRow struct {
	TotalUploads       int64       `json:"total_uploads"`
	SuccessfulUploads  int64       `json:"successful_uploads"`
	FailedUploads      int64       `json:"failed_uploads"`
	ProcessingUploads  int64       `json:"processing_uploads"`
	PendingUploads     int64       `json:"pending_uploads"`
	TotalRowsProcessed interface{} `json:"total_rows_processed"`
	TotalCategorized   interface{} `json:"total_categorized"`
	AvgAccuracyPercent interface{} `json:"avg_accuracy_percent"`
	AvgProcessingMs    interface{} `json:"avg_processing_ms"`
}

// Get aggregated upload statistics for a user
func (q *Queries) GetUploadStatsByUser(ctx context.Context, userID pgtype.UUID) (GetUploadStatsByUserRow, error) {
	row := q.db.QueryRow(ctx, getUploadStatsByUser, userID)
	var i GetUploadStatsByUserRow
	err := row.Scan(
		&i.TotalUploads,
		&i.SuccessfulUploads,
		&i.FailedUploads,
		&i.ProcessingUploads,
		&i.PendingUploads,
		&i.TotalRowsProcessed,
		&i.TotalCategorized,
		&i.AvgAccuracyPercent,
		&i.AvgProcessingMs,
	)
	return i, err
}

const getUserUploadHistory = `-- name: GetUserUploadHistory :many
SELECT id, user_id, filename, file_key, file_size_bytes, file_hash, bank_type, status, error_message, processing_started_at, processing_completed_at, total_rows, parsed_rows, categorized_rows, duplicate_rows, error_rows, accuracy_percent, processing_duration_ms, created_at, updated_at FROM upload_history
WHERE user_id = $1
ORDER BY created_at DESC
LIMIT $2 OFFSET $3
`

type GetUserUploadHistoryParams struct {
	UserID pgtype.UUID `json:"user_id"`
	Limit  int32       `json:"limit"`
	Offset int32       `json:"offset"`
}

// Get paginated upload history for a user
func (q *Queries) GetUserUploadHistory(ctx context.Context, arg GetUserUploadHistoryParams) ([]UploadHistory, error) {
	rows, err := q.db.Query(ctx, getUserUploadHistory, arg.UserID, arg.Limit, arg.Offset)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []UploadHistory{}
	for rows.Next() {
		var i UploadHistory
		if err := rows.Scan(
			&i.ID,
			&i.UserID,
			&i.Filename,
			&i.FileKey,
			&i.FileSizeBytes,
			&i.FileHash,
			&i.BankType,
			&i.Status,
			&i.ErrorMessage,
			&i.ProcessingStartedAt,
			&i.ProcessingCompletedAt,
			&i.TotalRows,
			&i.ParsedRows,
			&i.CategorizedRows,
			&i.DuplicateRows,
			&i.ErrorRows,
			&i.AccuracyPercent,
			&i.ProcessingDurationMs,
			&i.CreatedAt,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getUserUploadHistoryWithStats = `-- name: GetUserUploadHistoryWithStats :many
SELECT
    uh.id, uh.user_id, uh.filename, uh.file_key, uh.file_size_bytes, uh.file_hash, uh.bank_type, uh.status, uh.error_message, uh.processing_started_at, uh.processing_completed_at, uh.total_rows, uh.parsed_rows, uh.categorized_rows, uh.duplicate_rows, uh.error_rows, uh.accuracy_percent, uh.processing_duration_ms, uh.created_at, uh.updated_at,
    COUNT(DISTINCT t.id) as transaction_count
FROM upload_history uh
LEFT JOIN transactions t ON uh.id = t.upload_id
WHERE uh.user_id = $1
GROUP BY uh.id
ORDER BY uh.created_at DESC
LIMIT $2 OFFSET $3
`

type GetUserUploadHistoryWithStatsParams struct {
	UserID pgtype.UUID `json:"user_id"`
	Limit  int32       `json:"limit"`
	Offset int32       `json:"offset"`
}

type GetUserUploadHistoryWithStatsRow struct {
	ID                    pgtype.UUID        `json:"id"`
	UserID                pgtype.UUID        `json:"user_id"`
	Filename              string             `json:"filename"`
	FileKey               string             `json:"file_key"`
	FileSizeBytes         pgtype.Int8        `json:"file_size_bytes"`
	FileHash              pgtype.Text        `json:"file_hash"`
	BankType              pgtype.Text        `json:"bank_type"`
	Status                UploadStatus       `json:"status"`
	ErrorMessage          pgtype.Text        `json:"error_message"`
	ProcessingStartedAt   pgtype.Timestamptz `json:"processing_started_at"`
	ProcessingCompletedAt pgtype.Timestamptz `json:"processing_completed_at"`
	TotalRows             pgtype.Int4        `json:"total_rows"`
	ParsedRows            pgtype.Int4        `json:"parsed_rows"`
	CategorizedRows       pgtype.Int4        `json:"categorized_rows"`
	DuplicateRows         pgtype.Int4        `json:"duplicate_rows"`
	ErrorRows             pgtype.Int4        `json:"error_rows"`
	AccuracyPercent       pgtype.Numeric     `json:"accuracy_percent"`
	ProcessingDurationMs  pgtype.Int4        `json:"processing_duration_ms"`
	CreatedAt             pgtype.Timestamptz `json:"created_at"`
	UpdatedAt             pgtype.Timestamptz `json:"updated_at"`
	TransactionCount      int64              `json:"transaction_count"`
}

// Get upload history with transaction counts
func (q *Queries) GetUserUploadHistoryWithStats(ctx context.Context, arg GetUserUploadHistoryWithStatsParams) ([]GetUserUploadHistoryWithStatsRow, error) {
	rows, err := q.db.Query(ctx, getUserUploadHistoryWithStats, arg.UserID, arg.Limit, arg.Offset)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []GetUserUploadHistoryWithStatsRow{}
	for rows.Next() {
		var i GetUserUploadHistoryWithStatsRow
		if err := rows.Scan(
			&i.ID,
			&i.UserID,
			&i.Filename,
			&i.FileKey,
			&i.FileSizeBytes,
			&i.FileHash,
			&i.BankType,
			&i.Status,
			&i.ErrorMessage,
			&i.ProcessingStartedAt,
			&i.ProcessingCompletedAt,
			&i.TotalRows,
			&i.ParsedRows,
			&i.CategorizedRows,
			&i.DuplicateRows,
			&i.ErrorRows,
			&i.AccuracyPercent,
			&i.ProcessingDurationMs,
			&i.CreatedAt,
			&i.UpdatedAt,
			&i.TransactionCount,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getUserUploadStatsFromView = `-- name: GetUserUploadStatsFromView :one
SELECT user_id, email, total_uploads, successful_uploads, failed_uploads, total_rows_processed, total_categorized, avg_accuracy_percent, avg_processing_ms, last_upload_at, total_transactions FROM user_upload_stats
WHERE user_id = $1
`

// Get cached user statistics from materialized view
func (q *Queries) GetUserUploadStatsFromView(ctx context.Context, userID pgtype.UUID) (UserUploadStat, error) {
	row := q.db.QueryRow(ctx, getUserUploadStatsFromView, userID)
	var i UserUploadStat
	err := row.Scan(
		&i.UserID,
		&i.Email,
		&i.TotalUploads,
		&i.SuccessfulUploads,
		&i.FailedUploads,
		&i.TotalRowsProcessed,
		&i.TotalCategorized,
		&i.AvgAccuracyPercent,
		&i.AvgProcessingMs,
		&i.LastUploadAt,
		&i.TotalTransactions,
	)
	return i, err
}

const insertTransactionWithDuplicateCheck = `-- name: InsertTransactionWithDuplicateCheck :one
INSERT INTO transactions (
    user_id,
    upload_id,
    txn_date,
    description,
    amount,
    txn_type,
    category,
    is_reviewed,
    raw_data
) VALUES (
    $1, $2, $3, $4, $5, $6, $7, $8, $9
)
ON CONFLICT (user_id, txn_date, description, amount) DO NOTHING
RETURNING id, user_id, txn_date, description, amount, txn_type, category, is_reviewed, raw_data, created_at, updated_at, upload_id
`

type InsertTransactionWithDuplicateCheckParams struct {
	UserID      pgtype.UUID    `json:"user_id"`
	UploadID    pgtype.UUID    `json:"upload_id"`
	TxnDate     pgtype.Date    `json:"txn_date"`
	Description string         `json:"description"`
	Amount      pgtype.Numeric `json:"amount"`
	TxnType     string         `json:"txn_type"`
	Category    pgtype.Text    `json:"category"`
	IsReviewed  bool           `json:"is_reviewed"`
	RawData     pgtype.Text    `json:"raw_data"`
}

// Insert single transaction with duplicate check
func (q *Queries) InsertTransactionWithDuplicateCheck(ctx context.Context, arg InsertTransactionWithDuplicateCheckParams) (Transaction, error) {
	row := q.db.QueryRow(ctx, insertTransactionWithDuplicateCheck,
		arg.UserID,
		arg.UploadID,
		arg.TxnDate,
		arg.Description,
		arg.Amount,
		arg.TxnType,
		arg.Category,
		arg.IsReviewed,
		arg.RawData,
	)
	var i Transaction
	err := row.Scan(
		&i.ID,
		&i.UserID,
		&i.TxnDate,
		&i.Description,
		&i.Amount,
		&i.TxnType,
		&i.Category,
		&i.IsReviewed,
		&i.RawData,
		&i.CreatedAt,
		&i.UpdatedAt,
		&i.UploadID,
	)
	return i, err
}

const refreshUserUploadStats = `-- name: RefreshUserUploadStats :exec

REFRESH MATERIALIZED VIEW CONCURRENTLY user_upload_stats
`

// ============================================
// Materialized View Refresh
// ============================================
// Refresh materialized view for user statistics
func (q *Queries) RefreshUserUploadStats(ctx context.Context) error {
	_, err := q.db.Exec(ctx, refreshUserUploadStats)
	return err
}

const startProcessingUpload = `-- name: StartProcessingUpload :one
UPDATE upload_history
SET
    status = 'processing',
    processing_started_at = NOW(),
    updated_at = NOW()
WHERE id = $1 AND status = 'pending'
RETURNING id, user_id, filename, file_key, file_size_bytes, file_hash, bank_type, status, error_message, processing_started_at, processing_completed_at, total_rows, parsed_rows, categorized_rows, duplicate_rows, error_rows, accuracy_percent, processing_duration_ms, created_at, updated_at
`

// Mark upload as processing and set start time
func (q *Queries) StartProcessingUpload(ctx context.Context, id pgtype.UUID) (UploadHistory, error) {
	row := q.db.QueryRow(ctx, startProcessingUpload, id)
	var i UploadHistory
	err := row.Scan(
		&i.ID,
		&i.UserID,
		&i.Filename,
		&i.FileKey,
		&i.FileSizeBytes,
		&i.FileHash,
		&i.BankType,
		&i.Status,
		&i.ErrorMessage,
		&i.ProcessingStartedAt,
		&i.ProcessingCompletedAt,
		&i.TotalRows,
		&i.ParsedRows,
		&i.CategorizedRows,
		&i.DuplicateRows,
		&i.ErrorRows,
		&i.AccuracyPercent,
		&i.ProcessingDurationMs,
		&i.CreatedAt,
		&i.UpdatedAt,
	)
	return i, err
}

const updateUploadStatistics = `-- name: UpdateUploadStatistics :one
UPDATE upload_history
SET
    parsed_rows = COALESCE($2, parsed_rows),
    categorized_rows = COALESCE($3, categorized_rows),
    duplicate_rows = COALESCE($4, duplicate_rows),
    error_rows = COALESCE($5, error_rows),
    updated_at = NOW()
WHERE id = $1
RETURNING id, user_id, filename, file_key, file_size_bytes, file_hash, bank_type, status, error_message, processing_started_at, processing_completed_at, total_rows, parsed_rows, categorized_rows, duplicate_rows, error_rows, accuracy_percent, processing_duration_ms, created_at, updated_at
`

type UpdateUploadStatisticsParams struct {
	ID              pgtype.UUID `json:"id"`
	ParsedRows      pgtype.Int4 `json:"parsed_rows"`
	CategorizedRows pgtype.Int4 `json:"categorized_rows"`
	DuplicateRows   pgtype.Int4 `json:"duplicate_rows"`
	ErrorRows       pgtype.Int4 `json:"error_rows"`
}

// Update only statistics (for partial updates during processing)
func (q *Queries) UpdateUploadStatistics(ctx context.Context, arg UpdateUploadStatisticsParams) (UploadHistory, error) {
	row := q.db.QueryRow(ctx, updateUploadStatistics,
		arg.ID,
		arg.ParsedRows,
		arg.CategorizedRows,
		arg.DuplicateRows,
		arg.ErrorRows,
	)
	var i UploadHistory
	err := row.Scan(
		&i.ID,
		&i.UserID,
		&i.Filename,
		&i.FileKey,
		&i.FileSizeBytes,
		&i.FileHash,
		&i.BankType,
		&i.Status,
		&i.ErrorMessage,
		&i.ProcessingStartedAt,
		&i.ProcessingCompletedAt,
		&i.TotalRows,
		&i.ParsedRows,
		&i.CategorizedRows,
		&i.DuplicateRows,
		&i.ErrorRows,
		&i.AccuracyPercent,
		&i.ProcessingDurationMs,
		&i.CreatedAt,
		&i.UpdatedAt,
	)
	return i, err
}

const updateUploadStatus = `-- name: UpdateUploadStatus :one
UPDATE upload_history
SET
    status = $2,
    error_message = $3,
    updated_at = NOW()
WHERE id = $1
RETURNING id, user_id, filename, file_key, file_size_bytes, file_hash, bank_type, status, error_message, processing_started_at, processing_completed_at, total_rows, parsed_rows, categorized_rows, duplicate_rows, error_rows, accuracy_percent, processing_duration_ms, created_at, updated_at
`

type UpdateUploadStatusParams struct {
	ID           pgtype.UUID  `json:"id"`
	Status       UploadStatus `json:"status"`
	ErrorMessage pgtype.Text  `json:"error_message"`
}

// Update upload status (for state transitions)
func (q *Queries) UpdateUploadStatus(ctx context.Context, arg UpdateUploadStatusParams) (UploadHistory, error) {
	row := q.db.QueryRow(ctx, updateUploadStatus, arg.ID, arg.Status, arg.ErrorMessage)
	var i UploadHistory
	err := row.Scan(
		&i.ID,
		&i.UserID,
		&i.Filename,
		&i.FileKey,
		&i.FileSizeBytes,
		&i.FileHash,
		&i.BankType,
		&i.Status,
		&i.ErrorMessage,
		&i.ProcessingStartedAt,
		&i.ProcessingCompletedAt,
		&i.TotalRows,
		&i.ParsedRows,
		&i.CategorizedRows,
		&i.DuplicateRows,
		&i.ErrorRows,
		&i.AccuracyPercent,
		&i.ProcessingDurationMs,
		&i.CreatedAt,
		&i.UpdatedAt,
	)
	return i, err
}
